# -*- coding: utf-8 -*-
"""FFN_with_vectorized_weights.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ott24BWBGPYakpO-B9xll7OhDcasFbR
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tqdm import tqdm_notebook
from sklearn.metrics import log_loss

data, labels = make_blobs(n_samples = 1000, n_features= 2, centers = 4)
sns.set()
plt.scatter(data[:,0], data[:,1], c=labels)

X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size = 0.2, random_state = 2)

enc = OneHotEncoder()
Y_train_OH = enc.fit_transform(Y_train.reshape(-1,1))
Y_test_OH = enc.fit_transform(Y_test.reshape(-1,1))

Y_train_OH[0]

class FF_net:
  def __init__(self,W1,W2):
    self.W1 = W1.copy()
    self.W2 = W2.copy()
    self.B1 = np.zeros((1,2))
    self.B2 = np.zeros((1,4))

  def sigmoid(self, x):
    return 1.0/(1.0 + np.exp(-x))

  def softmax(self, x):
    return np.exp(x)/np.sum(np.exp(x))

  def forward_pass(self, x):
    x = x.reshape(1,-1)
    self.A1 = np.matmul(x,self.W1) + self.B1
    self.H1 = self.sigmoid(self.A1)
    self.A2 = np.matmul(self.H1, self.W2) + self.B2
    self.H2 = self.softmax(self.A2)
    return self.H2

  def grad_sigmoid(self, x):
    return x*(1-x)
  
  def grad(self, x, y):
    x= x.reshape(1,-1)
    y= y.reshape(1,-1)
    self.forward_pass(x)

    self.dA2 = self.H2 - y
    
    self.dW2 = np.matmul(self.H1.T, self.dA2)
    self.dH1 = np.matmul(self.dA2, self.W2.T)
    self.dB2 = self.dA2
    self.dA1 = np.multiply(self.dH1, self.grad_sigmoid(self.H1))

    self.dW1 = np.matmul(x.T, self.dA1)
    self.dB1 = self.dA1

  def fit(self, X, Y, epochs = 100, lr= 0.1):
    loss = []
    for i in tqdm_notebook(range(epochs), total = epochs, unit="epochs"):
      dW1 = np.zeros((2,2))
      dW2 = np.zeros((2,4))
      dB1 = np.zeros((1,2))
      dB2 = np.zeros((1,4))

      for x,y in zip(X,Y):
        self.grad(x,y)
        dW1 += self.dW1
        dW2 += self.dW2
        dB1 += self.dB1
        dB2 += self.dB2
      
      rows = X.shape[0]
      self.W1 -= lr*dW1/rows
      self.W2 -= lr*dW2/rows
      self.B1 -= lr*dB1/rows
      self.B2 -= lr*dB2/rows
    
      Y_pred = self.predict(X)
      loss.append(log_loss(np.argmax(Y, axis=1), Y_pred))
    
    plt.plot(loss)

  def predict(self, X):
    Y_pred = []
    for x in X:
      pred = self.forward_pass(x)
      Y_pred.append(pred)
    return np.array(Y_pred).squeeze()

W1 = np.random.randn(2,2)
W2 = np.random.randn(2,4)
model = FF_net(W1, W2)
model.fit(X_train, Y_train_OH, epochs = 500, lr=0.01)

