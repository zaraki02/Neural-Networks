# -*- coding: utf-8 -*-
"""FFN_with_vectorized_weights_and_Input.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g2QFR__PW86PbXMwh8qvBz-ujoRSfJls
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tqdm import tqdm_notebook
from sklearn.metrics import log_loss

data, labels = make_blobs(n_samples = 1000, n_features= 2, centers = 4)
sns.set()
plt.scatter(data[:,0], data[:,1], c=labels)

X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size = 0.2, random_state = 2)

enc = OneHotEncoder()
Y_train_OH = enc.fit_transform(Y_train.reshape(-1,1))
Y_test_OH = enc.fit_transform(Y_test.reshape(-1,1))

Y_train_OH[0]

class FF_net:
  def __init__(self,W1,W2):
    self.W1 = W1.copy()
    self.W2 = W2.copy()
    self.B1 = np.zeros((1,2))
    self.B2 = np.zeros((1,4))

  def sigmoid(self, x):
    return 1.0/(1.0 + np.exp(-x))

  def softmax(self, x):
    return np.exp(x)/np.sum(np.exp(x))

  def forward_pass(self, X):
    #x = x.reshape(1,-1)
    self.A1 = np.matmul(X,self.W1) + self.B1
    self.H1 = self.sigmoid(self.A1)
    self.A2 = np.matmul(self.H1, self.W2) + self.B2
    self.H2 = self.softmax(self.A2)
    return self.H2

  def grad_sigmoid(self, X):
    return X*(1-X)
  
  def grad(self, X, Y):
    #x= x.reshape(1,-1)
    #y= y.reshape(1,-1)
    self.forward_pass(X)

    self.dA2 = self.H2 - Y
    
    self.dW2 = np.matmul(self.H1.T, self.dA2)
    self.dH1 = np.matmul(self.dA2, self.W2.T)
    self.dB2 = np.sum(self.dA2, axis=0).reshape(1,-1)
    self.dA1 = np.multiply(self.dH1, self.grad_sigmoid(self.H1))

    self.dW1 = np.matmul(X.T, self.dA1)
    self.dB1 = np.sum(self.dA1, axis=0).reshape(1,-1)

  def fit(self, X, Y, epochs = 100, lr= 0.1):
    loss = []
    for i in tqdm_notebook(range(epochs), total = epochs, unit="epochs"):
      
      self.grad(X,Y)      
      
      rows = X.shape[0]
      self.W1 -= lr*self.dW1/rows
      self.W2 -= lr*self.dW2/rows
      self.B1 -= lr*self.dB1/rows
      self.B2 -= lr*self.dB2/rows
    
      Y_pred = self.predict(X)
      loss.append(log_loss(np.argmax(Y, axis=1), Y_pred))
    
    plt.plot(loss)

  def predict(self, X):
    Y_pred = self.forward_pass(X)
    return np.array(Y_pred).squeeze()

W1 = np.random.randn(2,2)
W2 = np.random.randn(2,4)
model = FF_net(W1, W2)
model.fit(X_train, Y_train_OH, epochs = 1500, lr=0.01)

